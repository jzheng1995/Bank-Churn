data_dia = y_train
data_dia = y_train
data = x_train
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import zipfile
from zipfile import ZipFile
# opening the zip file in READ mode
file_name = "playground-series-s4e1.zip"
with ZipFile(file_name, 'r') as zip:
# printing all the contents of the zip file
zip.printdir()
# import
df_zip = zipfile.ZipFile(file_name)
train = pd.read_csv(df_zip.open('train.csv'))
test = pd.read_csv(df_zip.open('test.csv'))
sample_submission = pd.read_csv(df_zip.open('sample_submission.csv'))
train.info()
# drop id columns
train.drop(["id","CustomerId","Surname"],axis = 1, inplace = True)
# features
x_train = train.drop(["Exited"], axis = 1)
# label
y_train = train.Exited
y_train.value_counts()
fig_exit = sns.barplot(x = y_train.value_counts().index, y = y_train.value_counts().values)
plt.show()
data_dia = y_train
data = x_train
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="exited",
data = pd.melt(data,id_vars="exited",
var_name="features",
value_name='value')
plt.figure(figsize=(10,10))
value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
sns.violinplot(x="features", y="value", hue="exited", data=data,split=True, inner="quart")
data
data = x_train.select_dtypes(include = "number")
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="exited",
var_name="features",
value_name='value')
data
data = pd.melt(data,id_vars="Exited",
var_name="features",
value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="exited", data=data,split=True, inner="quart")
sns.violinplot(x="features", y="value", hue="Exited", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()
data
data_dia = y_train
data = x_train.select_dtypes(include = "number")
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="Exited",
var_name="features",
value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="Exited", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()
y_train
x_train
data
data
plt.figure(figsize=(10,10))
data_dia = y_train
data = x_train.select_dtypes(include = "number")
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="Exited",
var_name="features",
value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="Exited", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()
data
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data.nunique()
data
data.info()
plt.show()
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(x_train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
data_dia = y_train
data_num = x_train.select_dtypes(include = "number")
data_n_2 = (data_num - data_num.mean()) / (data_num.std())              # standardization
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="Exited",
var_name="features",
value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="Exited", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()
sns.heatmap(data_num.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()
#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data_num.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()
data_dia = y_train
data_num = x_train.select_dtypes(include = "number")
data_n_2 = (data_num - data_num.mean()) / (data_num.std())              # standardization
data = pd.concat([y_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="Exited",
var_name="features",
value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="Exited", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()
data
#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()
#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data_num.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
# This sets the yticks "upright" with 0, as opposed to sideways with 90.
plt.yticks(rotation=0)
plt.show()
#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data_num.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
# This sets the yticks "upright" with 0, as opposed to sideways with 90.
plt.yticks(rotation=0)
plt.xticks(rotation=90)
plt.show()
clf_rf = RandomForestClassifier(random_state=42)
clr_rf = clf_rf.fit(x_train,y_train)
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score,confusion_matrix
from sklearn.metrics import accuracy_score
#random forest classifier with n_estimators=10 (default)
clf_rf = RandomForestClassifier(random_state=42)
clr_rf = clf_rf.fit(x_train,y_train)
clf_rf_rfe = RandomForestClassifier()
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation
rfecv = rfecv.fit(x_train, y_train)
from sklearn.feature_selection import RFECV
# The "accuracy" scoring is proportional to the number of correct classifications
clf_rf_rfe = RandomForestClassifier()
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation
rfecv = rfecv.fit(x_train, y_train)
from sklearn.feature_selection import RFECV
clf_rf_rfe = RandomForestClassifier()
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation
rfecv = rfecv.fit(x_train, y_train)
clf_rf_rfe = RandomForestClassifier()
rfecv = RFECV(estimator=clf_rf_rfe, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation
rfecv = rfecv.fit(x_train, y_train)
numerical_features = x_train.select_dtypes("number")
categorical_features = x_train.select_dtypes("object")
categorical_features
numerical_features
numerical_features = x_train.select_dtypes("number").values
categorical_features = x_train.select_dtypes("object").values
numerical_features
numerical_features = x_train.select_dtypes("number").columns.values
numerical_features
x_train.select_dtypes("number").columns[1]
numerical_features = x_train.select_dtypes("number").columns.values.tolist
numerical_features
numerical_features = list(x_train.select_dtypes("number").columns.values)
numerical_features
categorical_features = list(x_train.select_dtypes("object").values)
categorical_features = list(x_train.select_dtypes("object").columns.values)
categorical_features
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
from sklearn.impute import SimpleImputer
categorical_features = x_train.select_dtypes("object").columns
numerical_features = x_train.select_dtypes("number").columns
# numerical transformer
num_pipe = Pipeline(steps=[
("standardize", StandardScaler())
])
# categorical transformer
cat_pipe = Pipeline(steps=[
("ohe", OneHotEncoder(handle_unknown="ignore"))
])
preprocess = ColumnTransformer(
transformers=[
("numerical", num_pipe, numerical_columns),
("categorical", cat_pipe, categorical_columns),
],
remainder = "passthrough"
)
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
from sklearn.impute import SimpleImputer
cat_col = x_train.select_dtypes("object").columns
num_col = x_train.select_dtypes("number").columns
# numerical transformer
num_pipe = Pipeline(steps=[
("standardize", StandardScaler())
])
# categorical transformer
cat_pipe = Pipeline(steps=[
("ohe", OneHotEncoder(handle_unknown="ignore"))
])
preprocess = ColumnTransformer(
transformers=[
("numerical", num_pipe, num_col),
("categorical", cat_pipe, cat_col),
],
remainder = "passthrough"
)
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
lor = LogisticRegression()
xgb = XGBClassifier()
rf = RandomForestClassifier()
import xgboost
import lightgbm
import xgboost
import lightgbm
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
lor = LogisticRegression()
xgb = XGBClassifier()
rf = RandomForestClassifier()
lgbm = LGBMClassifier()
from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score
test.drop(["id","CustomerId","Surname"],axis = 1, inplace = True)
# features
x_test = test.drop(["Exited"], axis = 1)
# label
y_test = test.Exited
from sklearn.model_selection import train_test_split
X_data, X_label, y_data,y_label =  train_test_split( test_size=0.33,  random_state=42,  stratify="Exited")
X_data, X_label, y_data,y_label =  train_test_split(x_train, y_train, test_size=0.33,  random_state=42,  stratify="Exited")
X_data, X_label, y_data,y_label =  train_test_split(x_train, y_train, test_size=0.33,  random_state=42)
X_data, X_label, y_data,y_label =  train_test_split(x_train, y_train, test_size=0.33,  random_state=42, stratify = y_train)
X_data, X_valid, y_data,y_valid =  train_test_split(x_train, y_train, test_size=0.33,  random_state=42, stratify = y_train)
def Train_Model(clf,X_data,y_train,X_valid,y_valid):
pipeline = Pipeline(steps=[
('column_tran',preprocess),
('model',clf)
])
pipeline.fit(X_data,y_data)
y_pred = pipeline.predict(x_valid)
acc = accuracy_score(y_valid,y_pred)
ps = precision_score(y_valid,y_pred)
rec = recall_score(y_valid,y_pred)
f1 = f1_score(y_valid,y_pred)
return acc , ps ,rec, f1
clfs = {
'lor':lor,
'rf':rf,
'xgb':xgb,
'lgbm':lgbm
}
from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score
def Train_Model(clf,x_train,y_train,x_test,y_test):
pipeline = Pipeline(steps=[
('column_tran',preprocess),
('model',clf)
])
pipeline.fit(x_train,y_train)
y_pred = pipeline.predict(x_test)
acc = accuracy_score(y_test,y_pred)
ps = precision_score(y_test,y_pred)
rec = recall_score(y_test,y_pred)
f1 = f1_score(y_test,y_pred)
return acc , ps ,rec, f1
acc_score = []
pre_score = []
recall = []
f1s = []
for key,value in clfs.items():
acc , ps,rs,f1 = Train_Model(value,X_data,y_data,X_label,y_label)
print("name:",key)
print('ac:',acc)
print('ps :',ps)
print('rs:',rs)
print('f1:',f1)
acc_score.append(acc)
pre_score.append(ps)
recall.append(rs)
f1s.append(f1)
model_df = pd.DataFrame({'Algoritham':clfs.keys(),'Accuracy Score':acc_score,'Precision Score':pre_score,'Recall Score':recall,'F1 Score':f1s}).sort_values('Accuracy Score',ascending=False)
model_df
from sklearn.ensemble import VotingClassifier
vote = VotingClassifier(estimators=[
('xgb',xgb),
('random forest',rf),
('lgbm',lgbm)
])
vote_pipeline = Pipeline(steps=[
('colums_tran',preprocess),
('model',vote)
])
vote_pipeline.fit(X_data,y_label)
vote_pipeline.fit(X_data,y_data)
from sklearn.ensemble import VotingClassifier
vote = VotingClassifier(estimators=[
('xgb',xgb),
('random forest',rf),
('lgbm',lgbm)], voting = 'hard')
vote_pipeline = Pipeline(steps=[
('colums_tran',preprocess),
('model',vote)
])
vote_pipeline.fit(X_data,y_data)
import joblib
joblib.dump(vote_pipeline, 'vote_classifier', compress=9)
vote_classifier.predict_proba(test)
vote_pipe.predict_proba(test)
vote_pipeline.predict_proba(test)
from sklearn.ensemble import VotingClassifier
vote = VotingClassifier(estimators=[
('xgb',xgb),
('random forest',rf),
('lgbm',lgbm)], voting = 'soft')
vote_pipeline = Pipeline(steps=[
('colums_tran',preprocess),
('model',vote)
])
vote_pipeline.fit(X_data,y_data)
vote_pipeline.predict_proba(test)
submission = vote_pipeline.predict_proba(test)
test
test.id
test
test.columns
test = pd.read_csv(df_zip.open('test.csv'))
test.columns
reticulate::repl_python()
library(reticulate)
library(tidyverse)
library(kableExtra)
# tabulate python tables
pytable <- function(object, row = 0){
if(row == 0){
py[object] %>% kbl() %>%
kable_material_dark()
} else {
py[object] %>% head(row) %>% kbl() %>%
kable_material_dark()
}}
train = py$train
train %>% glimpse()
train %>% colnames()
library(psych)
train %>% describe()
(entries_per_customer <- train %>% group_by(CustomerId) %>% tally())
(entries_per_customer %>% group_by(n) %>% tally())
n_4 <- entries_per_customer %>% filter(n == 4)
train %>% filter(CustomerId %in% n_4$CustomerId) %>% arrange(CustomerId) %>% head()
reticulate::repl_python()
library(reticulate)
library(tidyverse)
library(kableExtra)
# tabulate python tables
pytable <- function(object, row = 0){
if(row == 0){
py[object] %>% kbl() %>%
kable_material_dark()
} else {
py[object] %>% head(row) %>% kbl() %>%
kable_material_dark()
}}
reticulate::repl_python()
library(reticulate)
library(tidyverse)
library(kableExtra)
# tabulate python tables
pytable <- function(object, row = 0){
if(row == 0){
py[object] %>% kbl() %>%
kable_material_dark()
} else {
py[object] %>% head(row) %>% kbl() %>%
kable_material_dark()
}}
train = py$train
train %>% glimpse()
train %>% colnames()
library(psych)
train %>% describe()
(entries_per_customer <- train %>% group_by(CustomerId) %>% tally())
(entries_per_customer %>% group_by(n) %>% tally())
n_4 <- entries_per_customer %>% filter(n == 4)
train %>% filter(CustomerId %in% n_4$CustomerId) %>% arrange(CustomerId) %>% head()
reticulate::repl_python()
"model_df" %>% pytable()
reticulate::repl_python()
